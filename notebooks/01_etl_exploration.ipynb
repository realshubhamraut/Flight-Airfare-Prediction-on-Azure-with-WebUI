{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23009221",
   "metadata": {},
   "source": [
    "### PySpark ETL Pipeline - Exploration & Development\n",
    "\n",
    "This notebook documents the exploration and development of our PySpark-based ETL pipeline for the Flight Airfare Prediction project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2de961",
   "metadata": {},
   "source": [
    "### 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83656dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/proxim/projects/Flight-Airfare-Prediction-on-Azure-with-WebUI\n"
     ]
    }
   ],
   "source": [
    "# Standard imports\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent if 'notebooks' in str(Path.cwd()) else Path.cwd()\n",
    "sys.path.insert(0, str(project_root))\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f19e648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# PySpark imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ffc9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FlightETL-Exploration\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set log level\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"App Name: {spark.sparkContext.appName}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749eb5f5",
   "metadata": {},
   "source": [
    "## 2. Data Exploration with PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee8c6309",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load raw data\u001b[39;00m\n\u001b[32m      2\u001b[39m data_path = \u001b[38;5;28mstr\u001b[39m(project_root / \u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m / \u001b[33m\"\u001b[39m\u001b[33mtrain.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m raw_df = \u001b[43mspark\u001b[49m.read \\\n\u001b[32m      5\u001b[39m     .option(\u001b[33m\"\u001b[39m\u001b[33mheader\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m\"\u001b[39m) \\\n\u001b[32m      6\u001b[39m     .option(\u001b[33m\"\u001b[39m\u001b[33minferSchema\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m\"\u001b[39m) \\\n\u001b[32m      7\u001b[39m     .csv(data_path)\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mraw_df.count()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m rows from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mColumns: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mraw_df.columns\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "# Load raw data\n",
    "data_path = str(project_root / \"data\" / \"train.csv\")\n",
    "\n",
    "raw_df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(data_path)\n",
    "\n",
    "print(f\"Loaded {raw_df.count()} rows from {data_path}\")\n",
    "print(f\"Columns: {raw_df.columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e15b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema inspection\n",
    "raw_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d03709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data preview\n",
    "raw_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c360567d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data statistics\n",
    "raw_df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3722da11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null analysis\n",
    "null_counts = []\n",
    "for col in raw_df.columns:\n",
    "    null_count = raw_df.filter(F.col(col).isNull()).count()\n",
    "    null_counts.append((col, null_count, null_count / raw_df.count() * 100))\n",
    "\n",
    "null_df = pd.DataFrame(null_counts, columns=['Column', 'Null_Count', 'Null_Percentage'])\n",
    "print(\"Null Analysis:\")\n",
    "print(null_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c30d6a",
   "metadata": {},
   "source": [
    "## 3. Bronze Layer - Raw Data Ingestion\n",
    "\n",
    "The Bronze layer stores raw data exactly as received from the source, with added metadata for lineage tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b629d38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment: Adding ingestion metadata\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "\n",
    "batch_id = str(uuid.uuid4())[:8]\n",
    "\n",
    "bronze_df = raw_df \\\n",
    "    .withColumn(\"_ingestion_timestamp\", F.current_timestamp()) \\\n",
    "    .withColumn(\"_source_file\", F.lit(data_path)) \\\n",
    "    .withColumn(\"_batch_id\", F.lit(batch_id)) \\\n",
    "    .withColumn(\"_ingestion_date\", F.current_date())\n",
    "\n",
    "print(f\"Bronze layer columns: {bronze_df.columns}\")\n",
    "bronze_df.select(\"_ingestion_timestamp\", \"_source_file\", \"_batch_id\").show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af602f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment: Schema definition for consistent data types\n",
    "BRONZE_SCHEMA = StructType([\n",
    "    StructField(\"Airline\", StringType(), True),\n",
    "    StructField(\"Date_of_Journey\", StringType(), True),\n",
    "    StructField(\"Source\", StringType(), True),\n",
    "    StructField(\"Destination\", StringType(), True),\n",
    "    StructField(\"Route\", StringType(), True),\n",
    "    StructField(\"Dep_Time\", StringType(), True),\n",
    "    StructField(\"Arrival_Time\", StringType(), True),\n",
    "    StructField(\"Duration\", StringType(), True),\n",
    "    StructField(\"Total_Stops\", StringType(), True),\n",
    "    StructField(\"Additional_Info\", StringType(), True),\n",
    "    StructField(\"Price\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "print(\"Bronze schema defined for consistent ingestion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38997a4",
   "metadata": {},
   "source": [
    "## 4. Silver Layer - Data Cleaning Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb28912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 1: Date parsing strategies\n",
    "print(\"Testing date parsing...\")\n",
    "print(f\"Sample dates: {raw_df.select('Date_of_Journey').distinct().take(5)}\")\n",
    "\n",
    "# Try different date formats\n",
    "date_test = raw_df.withColumn(\n",
    "    \"parsed_date\",\n",
    "    F.to_date(F.col(\"Date_of_Journey\"), \"d/M/yyyy\")\n",
    ")\n",
    "\n",
    "# Count successful parses\n",
    "successful = date_test.filter(F.col(\"parsed_date\").isNotNull()).count()\n",
    "print(f\"Successfully parsed: {successful}/{raw_df.count()} ({successful/raw_df.count()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff7bd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 2: Duration parsing\n",
    "print(\"Testing duration parsing...\")\n",
    "print(f\"Sample durations: {raw_df.select('Duration').distinct().take(10)}\")\n",
    "\n",
    "# Parse duration to minutes\n",
    "duration_test = raw_df.withColumn(\n",
    "    \"Duration_Hours\",\n",
    "    F.when(\n",
    "        F.col(\"Duration\").contains(\"h\"),\n",
    "        F.regexp_extract(F.col(\"Duration\"), r\"(\\d+)h\", 1).cast(\"int\")\n",
    "    ).otherwise(0)\n",
    ").withColumn(\n",
    "    \"Duration_Mins\",\n",
    "    F.when(\n",
    "        F.col(\"Duration\").contains(\"m\"),\n",
    "        F.regexp_extract(F.col(\"Duration\"), r\"(\\d+)m\", 1).cast(\"int\")\n",
    "    ).otherwise(0)\n",
    ").withColumn(\n",
    "    \"Duration_Minutes\",\n",
    "    F.col(\"Duration_Hours\") * 60 + F.col(\"Duration_Mins\")\n",
    ")\n",
    "\n",
    "duration_test.select(\"Duration\", \"Duration_Hours\", \"Duration_Mins\", \"Duration_Minutes\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bab355a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 3: Stops parsing\n",
    "print(\"Testing stops parsing...\")\n",
    "print(f\"Unique stops: {raw_df.select('Total_Stops').distinct().collect()}\")\n",
    "\n",
    "stops_test = raw_df.withColumn(\n",
    "    \"Total_Stops_Num\",\n",
    "    F.when(F.col(\"Total_Stops\") == \"non-stop\", 0)\n",
    "     .when(F.col(\"Total_Stops\") == \"1 stop\", 1)\n",
    "     .when(F.col(\"Total_Stops\") == \"2 stops\", 2)\n",
    "     .when(F.col(\"Total_Stops\") == \"3 stops\", 3)\n",
    "     .when(F.col(\"Total_Stops\") == \"4 stops\", 4)\n",
    "     .otherwise(F.regexp_extract(F.col(\"Total_Stops\"), r\"(\\d+)\", 1).cast(\"int\"))\n",
    ")\n",
    "\n",
    "stops_test.groupBy(\"Total_Stops\", \"Total_Stops_Num\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd5cd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 4: String cleaning\n",
    "print(\"Testing string standardization...\")\n",
    "\n",
    "# Check for whitespace issues\n",
    "clean_test = raw_df.withColumn(\n",
    "    \"Airline_Clean\",\n",
    "    F.trim(F.col(\"Airline\"))\n",
    ").withColumn(\n",
    "    \"Source_Clean\",\n",
    "    F.initcap(F.trim(F.col(\"Source\")))\n",
    ")\n",
    "\n",
    "# Compare before/after\n",
    "differences = clean_test.filter(\n",
    "    F.col(\"Airline\") != F.col(\"Airline_Clean\")\n",
    ").count()\n",
    "print(f\"Records with whitespace issues in Airline: {differences}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7527a087",
   "metadata": {},
   "source": [
    "## 5. Gold Layer - Feature Engineering Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371bc1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create silver-like clean data for feature experiments\n",
    "silver_df = raw_df \\\n",
    "    .withColumn(\"Journey_Date\", F.to_date(F.col(\"Date_of_Journey\"), \"d/M/yyyy\")) \\\n",
    "    .withColumn(\"Journey_Day\", F.dayofmonth(\"Journey_Date\")) \\\n",
    "    .withColumn(\"Journey_Month\", F.month(\"Journey_Date\")) \\\n",
    "    .withColumn(\"Journey_Year\", F.year(\"Journey_Date\")) \\\n",
    "    .withColumn(\"Journey_DayOfWeek\", F.dayofweek(\"Journey_Date\")) \\\n",
    "    .withColumn(\"Dep_Hour\", F.split(F.col(\"Dep_Time\"), \":\").getItem(0).cast(\"int\")) \\\n",
    "    .withColumn(\"Dep_Minute\", F.split(F.col(\"Dep_Time\"), \":\").getItem(1).cast(\"int\")) \\\n",
    "    .withColumn(\"Duration_Minutes\", \n",
    "        F.when(F.col(\"Duration\").contains(\"h\"),\n",
    "            F.regexp_extract(F.col(\"Duration\"), r\"(\\d+)h\", 1).cast(\"int\") * 60\n",
    "        ).otherwise(0) +\n",
    "        F.when(F.col(\"Duration\").contains(\"m\"),\n",
    "            F.regexp_extract(F.col(\"Duration\"), r\"(\\d+)m\", 1).cast(\"int\")\n",
    "        ).otherwise(0)\n",
    "    ) \\\n",
    "    .withColumn(\"Total_Stops_Num\",\n",
    "        F.when(F.col(\"Total_Stops\") == \"non-stop\", 0)\n",
    "         .otherwise(F.regexp_extract(F.col(\"Total_Stops\"), r\"(\\d+)\", 1).cast(\"int\"))\n",
    "    )\n",
    "\n",
    "print(\"Silver data created for feature experiments\")\n",
    "silver_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6bbcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 5: Time-based features\n",
    "print(\"Creating time-based features...\")\n",
    "\n",
    "gold_df = silver_df \\\n",
    "    .withColumn(\"IsWeekend\",\n",
    "        F.when(F.col(\"Journey_DayOfWeek\").isin([1, 7]), 1).otherwise(0)\n",
    "    ) \\\n",
    "    .withColumn(\"IsMorningFlight\",\n",
    "        F.when((F.col(\"Dep_Hour\") >= 5) & (F.col(\"Dep_Hour\") < 12), 1).otherwise(0)\n",
    "    ) \\\n",
    "    .withColumn(\"IsAfternoonFlight\",\n",
    "        F.when((F.col(\"Dep_Hour\") >= 12) & (F.col(\"Dep_Hour\") < 17), 1).otherwise(0)\n",
    "    ) \\\n",
    "    .withColumn(\"IsEveningFlight\",\n",
    "        F.when((F.col(\"Dep_Hour\") >= 17) & (F.col(\"Dep_Hour\") < 21), 1).otherwise(0)\n",
    "    ) \\\n",
    "    .withColumn(\"IsNightFlight\",\n",
    "        F.when((F.col(\"Dep_Hour\") >= 21) | (F.col(\"Dep_Hour\") < 5), 1).otherwise(0)\n",
    "    ) \\\n",
    "    .withColumn(\"Is_Direct\",\n",
    "        F.when(F.col(\"Total_Stops_Num\") == 0, 1).otherwise(0)\n",
    "    )\n",
    "\n",
    "# Verify feature distributions\n",
    "print(\"\\nFeature distributions:\")\n",
    "gold_df.groupBy(\"IsMorningFlight\", \"IsAfternoonFlight\", \"IsEveningFlight\", \"IsNightFlight\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d94a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 6: Route-based features\n",
    "print(\"Creating route features...\")\n",
    "\n",
    "gold_df = gold_df \\\n",
    "    .withColumn(\"Route_Segments\",\n",
    "        F.when(F.col(\"Route\").isNotNull(),\n",
    "            F.size(F.split(F.col(\"Route\"), \" → \"))\n",
    "        ).otherwise(2)\n",
    "    ) \\\n",
    "    .withColumn(\"City_Pair\",\n",
    "        F.concat_ws(\"_\", F.col(\"Source\"), F.col(\"Destination\"))\n",
    "    )\n",
    "\n",
    "# Route complexity analysis\n",
    "print(\"\\nRoute complexity distribution:\")\n",
    "gold_df.groupBy(\"Route_Segments\").agg(\n",
    "    F.count(\"*\").alias(\"count\"),\n",
    "    F.mean(\"Price\").alias(\"avg_price\"),\n",
    "    F.mean(\"Duration_Minutes\").alias(\"avg_duration\")\n",
    ").orderBy(\"Route_Segments\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80d1d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 7: Statistical features (price aggregations)\n",
    "print(\"Creating statistical features...\")\n",
    "\n",
    "# Average price by airline\n",
    "airline_stats = gold_df.groupBy(\"Airline\").agg(\n",
    "    F.mean(\"Price\").alias(\"Price_mean_by_airline\"),\n",
    "    F.stddev(\"Price\").alias(\"Price_std_by_airline\")\n",
    ")\n",
    "\n",
    "# Average price by route\n",
    "route_stats = gold_df.groupBy(\"City_Pair\").agg(\n",
    "    F.mean(\"Price\").alias(\"Price_mean_by_route\"),\n",
    "    F.count(\"*\").alias(\"Route_frequency\")\n",
    ")\n",
    "\n",
    "# Join back to main dataframe\n",
    "gold_df = gold_df.join(airline_stats, on=\"Airline\", how=\"left\")\n",
    "gold_df = gold_df.join(route_stats, on=\"City_Pair\", how=\"left\")\n",
    "\n",
    "# Price deviation from airline mean\n",
    "gold_df = gold_df.withColumn(\n",
    "    \"Price_Deviation_Airline\",\n",
    "    F.when(F.col(\"Price_std_by_airline\") > 0,\n",
    "        (F.col(\"Price\") - F.col(\"Price_mean_by_airline\")) / F.col(\"Price_std_by_airline\")\n",
    "    ).otherwise(0)\n",
    ")\n",
    "\n",
    "print(\"Statistical features created\")\n",
    "gold_df.select(\"Airline\", \"Price\", \"Price_mean_by_airline\", \"Price_Deviation_Airline\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301fab6c",
   "metadata": {},
   "source": [
    "## 6. Feature Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a339b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Pandas for correlation analysis\n",
    "numeric_cols = [\n",
    "    \"Journey_Day\", \"Journey_Month\", \"Dep_Hour\", \"Dep_Minute\",\n",
    "    \"Duration_Minutes\", \"Total_Stops_Num\", \"IsWeekend\",\n",
    "    \"IsMorningFlight\", \"IsEveningFlight\", \"Is_Direct\",\n",
    "    \"Route_Segments\", \"Price\"\n",
    "]\n",
    "\n",
    "# Sample for visualization\n",
    "sample_pd = gold_df.select(numeric_cols).sample(0.3).toPandas()\n",
    "\n",
    "# Correlation matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "correlation = sample_pd.corr()\n",
    "sns.heatmap(correlation, annot=True, cmap='coolwarm', center=0, fmt='.2f')\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cd589f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Price correlations\n",
    "print(\"Correlation with Price:\")\n",
    "price_corr = correlation['Price'].sort_values(ascending=False)\n",
    "print(price_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8a64c8",
   "metadata": {},
   "source": [
    "## 7. Conclusions & Next Steps\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Date Parsing**: Format `d/M/yyyy` works correctly for all records\n",
    "2. **Duration**: Regex extraction handles all variations including edge cases\n",
    "3. **Stops**: Categorical to numeric mapping is straightforward\n",
    "4. **Time Features**: Morning/evening flight indicators show correlation with price\n",
    "5. **Route Complexity**: More segments = higher price (correlation confirmed)\n",
    "\n",
    "### Medallion Architecture Benefits:\n",
    "- Bronze: Preserves raw data for audit/replay\n",
    "- Silver: Clean, typed data for reliable transformations\n",
    "- Gold: ML-ready features with no additional preprocessing needed\n",
    "\n",
    "### Recommended Features for ML:\n",
    "- Duration_Minutes\n",
    "- Total_Stops_Num\n",
    "- Time-based flags (IsWeekend, IsMorningFlight, etc.)\n",
    "- Statistical features (Price_mean_by_airline, Price_mean_by_route)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711de30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "spark.stop()\n",
    "print(\"✓ Spark session stopped\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
